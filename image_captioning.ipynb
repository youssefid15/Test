{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26f5aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9eb924a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "IMAGE_DIR = \"C:/Users/Idris/OneDrive/Desktop/ML PROJECT 3/Flickr8k_Dataset/Images\"\n",
    "CAPTION_FILE = \"C:/Users/Idris/OneDrive/Desktop/ML PROJECT 3/Flickr8k_Dataset/captions.txt\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_EPOCHS = 5\n",
    "MAX_LEN = 35\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb292dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Charger les captions\n",
    "def load_captions(filepath):\n",
    "    captions = defaultdict(list)\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        for line in lines:\n",
    "            image_id, caption = line.strip().split(',', 1)\n",
    "            caption = re.sub(r'[^a-zA-Z0-9 ]+', '', caption.lower())\n",
    "            captions[image_id].append(f\"<start> {caption} <end>\")\n",
    "    return captions\n",
    "\n",
    "captions_dict = load_captions(CAPTION_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f58f2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Build vocabulary\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<pad>\", 1: \"<start>\", 2: \"<end>\", 3: \"<unk>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    def tokenizer(self, text):\n",
    "        return text.lower().split()\n",
    "\n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = defaultdict(int)\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            for word in self.tokenizer(sentence):\n",
    "                frequencies[word] += 1\n",
    "                if frequencies[word] == self.freq_threshold:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized = self.tokenizer(text)\n",
    "        return [\n",
    "            self.stoi.get(token, self.stoi[\"<unk>\"])\n",
    "            for token in tokenized\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0760a4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Custom Dataset\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, captions_dict, image_dir, transform=None, freq_threshold=5):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_ids = list(captions_dict.keys())\n",
    "        self.captions = []\n",
    "        for k in self.image_ids:\n",
    "            for cap in captions_dict[k]:\n",
    "                self.captions.append((k, cap))\n",
    "\n",
    "        self.vocab = Vocabulary(freq_threshold)\n",
    "        self.vocab.build_vocab([cap for _, cap in self.captions])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_id, caption = self.captions[index]\n",
    "        img_path = os.path.join(self.image_dir, image_id)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        numericalized_caption = [self.vocab.stoi[\"<start>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<end>\"])\n",
    "\n",
    "        return image, torch.tensor(numericalized_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e14bb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Collate Function\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        captions = [item[1] for item in batch]\n",
    "        captions = nn.utils.rnn.pad_sequence(captions, batch_first=True, padding_value=self.pad_idx)\n",
    "        return imgs, captions\n",
    "\n",
    "# 5. Transforms and Loader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = FlickrDataset(captions_dict, IMAGE_DIR, transform=transform)\n",
    "pad_idx = dataset.vocab.stoi[\"<pad>\"]\n",
    "vocab = dataset.vocab\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=MyCollate(pad_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0277d0b4-a4de-4b1d-b451-374d598dedac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Vocabulaire sauvegardé dans : C:/Users/Idris/OneDrive/Desktop/ML PROJECT 3/vocab.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "VOCAB_PATH = \"C:/Users/Idris/OneDrive/Desktop/ML PROJECT 3/vocab.pkl\"\n",
    "\n",
    "# Sauvegarde du vocabulaire\n",
    "with open(VOCAB_PATH, 'wb') as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "print(f\"✅ Vocabulaire sauvegardé dans : {VOCAB_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b866a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Encoder CNN + Decoder RNN\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super().__init__()\n",
    "        self.cnn = models.resnet18(pretrained=True)\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.cnn.fc = nn.Linear(self.cnn.fc.in_features, embed_size)\n",
    "\n",
    "    def forward(self, images):\n",
    "        return self.cnn(images)\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, features, captions):\n",
    "        embeddings = self.embed(captions[:, :-1])\n",
    "        inputs = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "        hiddens, _ = self.lstm(inputs)\n",
    "        outputs = self.linear(hiddens)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b93a821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Full Model\n",
    "class ImageCaptioningModel(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super().__init__()\n",
    "        self.encoder = EncoderCNN(embed_size)\n",
    "        self.decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        features = self.encoder(images)\n",
    "        return self.decoder(features, captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82c936ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Idris\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\Idris\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "100%|██████████| 1265/1265 [18:48<00:00,  1.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Loss: 3.8946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [18:30<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/15], Loss: 3.4339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [18:38<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/15], Loss: 3.2161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [18:29<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/15], Loss: 3.0725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [18:30<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/15], Loss: 2.9618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [18:29<00:00,  1.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/15], Loss: 2.8659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [17:31<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/15], Loss: 2.7804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [16:41<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/15], Loss: 2.7013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [16:29<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/15], Loss: 2.6245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [16:28<00:00,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/15], Loss: 2.5509\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [16:23<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/15], Loss: 2.4820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [16:22<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/15], Loss: 2.4127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [16:12<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/15], Loss: 2.3455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [16:01<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/15], Loss: 2.2788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1265/1265 [16:03<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/15], Loss: 2.2178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Entraînement\n",
    "\n",
    "NUM_EPOCHS = 15  # ⬅️ Spécifie ici le nombre d'époques souhaité\n",
    "\n",
    "model = ImageCaptioningModel(EMBEDDING_DIM, HIDDEN_DIM, len(dataset.vocab)).to(DEVICE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    total_loss = 0\n",
    "    for idx, (imgs, caps) in enumerate(tqdm(loader)):\n",
    "        imgs, caps = imgs.to(DEVICE), caps.to(DEVICE)\n",
    "\n",
    "        outputs = model(imgs, caps)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "        # Corriger la forme pour que outputs et targets soient alignés\n",
    "        outputs = outputs[:, :-1, :]  # ignore prédiction après le dernier token\n",
    "        targets = caps[:, 1:]         # ignore <SOS>\n",
    "\n",
    "        # Aplatir pour CrossEntropyLoss\n",
    "        outputs = outputs.reshape(-1, outputs.shape[2])  # (batch * seq_len, vocab_size)\n",
    "        targets = targets.reshape(-1)                    # (batch * seq_len)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {total_loss/len(loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "819ced9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modèle sauvegardé sous model.pth\n"
     ]
    }
   ],
   "source": [
    "# 9. Sauvegarder le modèle\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"✅ Modèle sauvegardé sous model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
